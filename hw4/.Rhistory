tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
str(tele_norm)
set.seed(12345)
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm))
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
table(is.na(tele_small))
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
log_pred <- predict(log_model, tele_test, type="response")
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# model
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
knitr::opts_chunk$set(echo = TRUE)
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
str(tele_norm)
set.seed(12345)
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm))
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
table(is.na(tele_small))
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
log_pred <- predict(log_model, tele_test, type="response")
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# model
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
knitr::opts_chunk$set(echo = TRUE)
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
str(tele_norm)
set.seed(12345)
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm))
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
table(is.na(tele_small))
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
log_pred <- predict(log_model, tele_test, type="response")
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# model
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
set.seed(12345)
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# model
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
table(tele_norm$yyes)
knitr::opts_chunk$set(echo = TRUE)
# read data, string variables to factors
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# do Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
# data structure
str(tele_norm)
# check how many customer buy the service in the data set, 1 is for yes;
table(tele_norm$yyes)
set.seed(12345) # keep same seed everytime
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm)) # this will only keep the 25% of the data set and randomize them, because of the computing power;
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
# split the test and train data set in 0.2:0.8 ratio;
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
# check whether there is any NA value
table(is.na(tele_small))
# set up and train a logistic regression model by train data
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
# use the model to do prediction on our test data
log_pred <- predict(log_model, tele_test, type="response")
# get binary results by the condition whether the prediction results  >= 0.5, if yes, will be 1, if not will be 0
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
# confusionMatrix for model
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split for KNN, column 53 is our label/target column "yyes"
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
# build,train and predict using KNN model with k = 30
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
# confusionMatrix result
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
# build a stack/second level model using the previous logistic regression and KNN model, compared to the test true label value
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
# rename the column
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
# split the test and train data for the stack model using ratio 0.2:0.8
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# using decision tree model to do prediction and get results
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
# Build a Confusion Matrix
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
# build a CrossTable to easily calculate the profit
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
set.seed(12345)
# split the test and train data for the stack model using ratio 0.2:0.8
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# using decision tree model to do prediction and get results
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
# Build a Confusion Matrix
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
knitr::opts_chunk$set(echo = TRUE)
# read data, string variables to factors
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# do Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
# data structure
str(tele_norm)
# check how many customer buy the service in the data set, 1 is for yes;
table(tele_norm$yyes)
set.seed(12345) # keep same seed everytime
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm)) # this will only keep the 25% of the data set and randomize them, because of the computing power;
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
# split the test and train data set in 0.2:0.8 ratio;
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
# check whether there is any NA value
table(is.na(tele_small))
# set up and train a logistic regression model by train data
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
# use the model to do prediction on our test data
log_pred <- predict(log_model, tele_test, type="response")
# get binary results by the condition whether the prediction results  >= 0.5, if yes, will be 1, if not will be 0
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
# confusionMatrix for model
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split for KNN, column 53 is our label/target column "yyes"
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
# build,train and predict using KNN model with k = 30
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
# confusionMatrix result
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
# build a stack/second level model using the previous logistic regression and KNN model, compared to the test true label value
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
# rename the column
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
# split the test and train data for the stack model using ratio 0.2:0.8
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# using decision tree model to do prediction and get results
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
# Build a Confusion Matrix
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
# build a CrossTable to easily calculate the profit
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
knitr::opts_chunk$set(echo = TRUE)
# read data, string variables to factors
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)
# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL
# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)
# Randomize (done already)
# do Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tele_norm <- as.data.frame(lapply(telemm, normalize))
# data structure
str(tele_norm)
# check how many customer buy the service in the data set, 1 is for yes;
table(tele_norm$yyes)
set.seed(12345) # keep same seed everytime
kill_rows <- sample(1:nrow(tele_norm), 0.25*nrow(tele_norm)) # this will only keep the 25% of the data set and randomize them, because of the computing power;
tele_small <- tele_norm[kill_rows, ]
str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
set.seed(12345)
# split the test and train data set in 0.2:0.8 ratio;
test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]
# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)
str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)
# check whether there is any NA value
table(is.na(tele_small))
# set up and train a logistic regression model by train data
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
# use the model to do prediction on our test data
log_pred <- predict(log_model, tele_test, type="response")
# get binary results by the condition whether the prediction results  >= 0.5, if yes, will be 1, if not will be 0
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)
# confusionMatrix for model
library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")
# label and data split for KNN, column 53 is our label/target column "yyes"
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]
KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]
# model
library(class)
# build,train and predict using KNN model with k = 30
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 30) # the optimal k having best accuracy
summary(knn_pred)
# evaluation
library(caret)
# confusionMatrix result
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
# build a stack/second level model using the previous logistic regression and KNN model, compared to the test true label value
stack_model <- data.frame(log_pred, knn_pred, tele_test$yyes)
# rename the column
colnames(stack_model) <- c("logi_reg", "KNN", "True_yyes")
summary(stack_model)
str(stack_model)
set.seed(12345)
# split the test and train data for the stack model using ratio 0.2:0.8
test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model))
stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]
str(stack_model_train)
# using decision tree model to do prediction and get results
library(C50)
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)
stack_model_pred <- predict(stack_model_DT, stack_model_test)
# Build a Confusion Matrix
confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")
library(gmodels)
# build a CrossTable to easily calculate the profit
CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
