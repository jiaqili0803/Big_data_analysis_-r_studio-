---
title: "hw4_submit"
author: "Ella Li (jiaqi li)"
date: "4/21/2022"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Read and clean data

```{r}
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# summary(tele)
# str(tele)

# drop useless columns
tele$X <- NULL
tele$duration <- NULL # We only should be using variables that we know before the call

tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1) # Changing pdays to a dummy and deleting pdays, 999 means client was not previously contacted
tele$pdays <- NULL

# convert factors to dummy variables (input into ANN has to be numeric)
telemm <- as.data.frame(model.matrix(~.-1, tele))
# telemm
table(telemm$yyes)

# Randomize (done already)



# Normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

tele_norm <- as.data.frame(lapply(telemm, normalize))

str(tele_norm)


```

# 2. Control model running time, so kill some columns
```{r}
set.seed(12345)
kill_rows <- sample(1:nrow(tele_norm), 0.05*nrow(tele_norm))
tele_small <- tele_norm[kill_rows, ]

str(tele_small)
summary(tele_small)
# tele_small is now a small but clean data set for futher analyse
```

# 3. Train Test split
```{r}
set.seed(12345)

test_set <- sample(1:nrow(tele_small), 0.2*nrow(tele_small))  
tele_train <- tele_small[-test_set, ]
tele_test <- tele_small[test_set, ]

# upsample to balance the sample size for 0 and 1 values for yyes for train data
# set.seed(12345)
# library(groupdata2)
# tele_train <- upsample(tele_train, cat_col = "yyes")
# table(tele_train$yyes)

str(tele_train)
summary(tele_train)
# str(tele_test)
# summary(tele_test)
# table(tele_train$yyes)

```

# 4. Check na
```{r}
table(is.na(tele_small)) 
```

# 5. Models
##  5.1 Logistic Regression model
```{r}
log_model <- glm(yyes ~., data = tele_train, family = "binomial")
log_pred <- predict(log_model, tele_test, type="response")
log_pred_binary <- ifelse(log_pred >= 0.5, 1, 0)

library(caret)
confusionMatrix(as.factor(log_pred_binary), as.factor(tele_test$yyes), positive = "1")

```


##  5.2 KNN model
```{r}
# label and data split
KNN_train_label <- tele_train[ , 53]
KNN_train <- tele_train[ , -53]

KNN_test_label <- tele_test[ , 53]
KNN_test <- tele_test[ , -53]

# model
library(class)
knn_pred <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 40) # train data rows square root = k
summary(knn_pred)

# evaluation
library(caret)
confusionMatrix(as.factor(knn_pred), as.factor(KNN_test_label), positive = "1")
```
##  5.3 ANN model
```{r}
library(neuralnet)

ann_model <- neuralnet(yyes ~., data = tele_train)
plot(ann_model)
ann_pred <- predict(ann_model, tele_test, type="response")
ann_pred_binary <- ifelse(ann_pred >= 0.5, 1, 0)

confusionMatrix(as.factor(ann_pred_binary), as.factor(tele_test$yyes), positive = "1")

# we need non-binary values for stack model for ANN and logi
```
##  5.4 Decision Tree model
```{r}
library(C50)
DT_model <- C5.0(as.factor(yyes) ~., data = tele_train) # C5.0 models require a factor outcome
# plot(DT_model)
# summary(DT_model)

DT_pred <- predict(DT_model, tele_test)

confusionMatrix(as.factor(DT_pred), as.factor(tele_test$yyes), positive = "1")

```
##  5.5 Random forest model
```{r}
library(randomForest)

rf_model <- randomForest(as.factor(yyes) ~., data = tele_train)
plot(rf_model)
rf_pred <- predict(rf_model, tele_test)

confusionMatrix(as.factor(rf_pred), as.factor(tele_test$yyes), positive = "1")

# find patterns
randomForest::importance(rf_model)
randomForest::varImpPlot(rf_model)

```
 # 6. Improve models
 
## 6.1 Try different K for KNN
```{r}
# when k=40, Accuracy=0.91
knn_pred_1 <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 37) 
#knn_pred_2 <- knn(train = KNN_train, test = KNN_test, cl = KNN_train_label, k = 58) 

confusionMatrix(as.factor(knn_pred_1), as.factor(KNN_test_label), positive = "1")
#confusionMatrix(as.factor(knn_pred_2), as.factor(KNN_test_label), positive = "1")
```
## 6.2 Use the Caret Train function to auto tune Decision Tree and pick the best
```{r}
# automated parameter tuning of C5.0 decision tree 
DT_tune_model <- train(as.factor(yyes) ~., data = tele_train, method = "C5.0")
DT_tune_model

# apply the best C5.0 candidate model to make predictions
DT_tune_pred <- predict(DT_tune_model, tele_test)
confusionMatrix(as.factor(DT_tune_pred), as.factor(tele_test$yyes), positive = "1")

```
## 6.3 Use the Caret Train function for auto-tune a boosted customized C5.0 Decision Tree
```{r}
# customized
grid_c50 <- expand.grid(.model = "tree",
                        .trials = c(10, 20, 30, 40),
                        .winnow = "FALSE")

ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# auto-tune model
DT_tune_model_customized <- train(as.factor(yyes) ~., data = tele_train, method = "C5.0",
                metric = "Kappa", trControl = ctrl,
               tuneGrid = grid_c50)

DT_tune_customized_pred <- predict(DT_tune_model_customized, tele_test)

# evaluation
confusionMatrix(as.factor(DT_tune_customized_pred), as.factor(tele_test$yyes), positive = "1")

```
# 7. Stacked Model
```{r}
stack_model <- data.frame(log_pred, knn_pred_1, ann_pred, DT_tune_pred, rf_pred, tele_test$yyes)
colnames(stack_model) <- c("logi_reg", "tuned_KNN", "ANN", "tuned_DT", "Random Forest", "True_yyes")

summary(stack_model)
str(stack_model)
  
```
# 8. Break stack model data frame into test and train, and do decision tree
```{r}
set.seed(12345)

test_set <- sample(1:nrow(stack_model), 0.2*nrow(stack_model)) 

stack_model_train <- stack_model[-test_set, ]
stack_model_test <- stack_model[test_set, ]

str(stack_model_train)

# model
stack_model_DT <- C5.0(as.factor(True_yyes)~., stack_model_train)

stack_model_pred <- predict(stack_model_DT, stack_model_test)

confusionMatrix(stack_model_pred, as.factor(stack_model_test$True_yyes), positive = "1")


```
```{r}
library(gmodels)

CrossTable(stack_model_pred, as.factor(stack_model_test$True_yyes), prop.chisq = F,prop.r = F, prop.c = F, prop.t = F)
```


# 9. Conclusion

## 9.1 Compare the models results

Logistic Regression model： Accuracy : 0.9075
KNN model： Accuracy : 0.91
ANN model：Accuracy : 0.8783
Decision Tree model：Accuracy : 0.8954
Random forest model：Accuracy : 0.9051
Improved KNN model：Accuracy : 0.9124
Improved DT_tune_model：Accuracy : 0.91
stack_model： Accuracy : 0.9268

We can find that our stack model have the highest accuracy.

## 9.2 How to improve the success of the TeleMarketing efforts?

I will improve the marketing call strategy by the stack model result, the Call Center will ONLY call those customers that stack model classify as likely to be successful, i.e. who are predicted as "will buy the service" rather than calling all the customers.  

According to the Cost Assumption, it costs 1 dollar to make each call, and every successful call results in a net profit of 8 dollar.
Because of the computing power limit, the sample is too small, but we can still find the pattern that: If we call all the customers, will lose (6*8-82)= -34 dollars, but if we follow the model, we do not call this 82 people, will save this 34 dollars for call center. If we have larger sample, the call center will even save more money and maximize their net profit.




